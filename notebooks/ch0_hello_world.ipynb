{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce091f46",
   "metadata": {},
   "source": [
    "# üîë Setting Up Your API Key (IMPORTANT!)\n",
    "\n",
    "Before running this notebook, you need to set up your Google Gemini API key securely.\n",
    "\n",
    "## üìù Step-by-Step Setup\n",
    "\n",
    "### 1. Get Your API Key\n",
    "Visit [Google AI Studio](https://aistudio.google.com/app/apikey) and create a free API key.\n",
    "\n",
    "### 2. Create a `.env` File\n",
    "In your project root directory (same folder as this notebook), create a file named `.env`:\n",
    "\n",
    "```bash\n",
    "# In terminal (Linux/Mac):\n",
    "touch .env\n",
    "\n",
    "# Or in Windows (PowerShell):\n",
    "New-Item .env\n",
    "```\n",
    "\n",
    "### 3. Add Your API Key to `.env`\n",
    "Open the `.env` file and add:\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=your_actual_api_key_here\n",
    "```\n",
    "\n",
    "Replace `your_actual_api_key_here` with your real API key!\n",
    "\n",
    "### 4. Verify `.env` is Protected\n",
    "Make sure `.env` is in your `.gitignore` file (already done for you!) so it never gets pushed to GitHub.\n",
    "\n",
    "---\n",
    "\n",
    "## üîí Security Best Practices\n",
    "\n",
    "‚úÖ **DO:**\n",
    "- Keep your API key in `.env` file\n",
    "- Add `.env` to `.gitignore`\n",
    "- Use environment variables in code\n",
    "\n",
    "‚ùå **DON'T:**\n",
    "- Hard-code API keys in notebooks\n",
    "- Commit `.env` to Git\n",
    "- Share API keys publicly\n",
    "\n",
    "---\n",
    "\n",
    "## üí° For Google Colab Users\n",
    "\n",
    "If you're using Google Colab, you can use Colab's built-in secrets feature:\n",
    "\n",
    "```python\n",
    "from google.colab import userdata\n",
    "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
    "```\n",
    "\n",
    "Or temporarily set it in the notebook (for learning only):\n",
    "```python\n",
    "import os\n",
    "os.environ['GOOGLE_API_KEY'] = 'your_key_here'  # Not recommended for production!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "Now run the cell below to load your API key and create your first LLM! üöÄ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variable\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "# Check if API key is loaded\n",
    "if not google_api_key:\n",
    "    raise ValueError(\n",
    "        \"‚ùå GOOGLE_API_KEY not found!\\n\"\n",
    "        \"Please create a .env file in the project root and add:\\n\"\n",
    "        \"GOOGLE_API_KEY=your_actual_api_key_here\\n\\n\"\n",
    "        \"Get your API key at: https://aistudio.google.com/app/apikey\"\n",
    "    )\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=google_api_key,\n",
    ")\n",
    "output = llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418ecee",
   "metadata": {},
   "source": [
    "# Understanding ChatGoogleGenerativeAI Parameters\n",
    "\n",
    "When we create an AI chatbot using Google's Gemini model, we need to configure it with several parameters. Let's understand each one in simple terms:\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Core Parameters\n",
    "\n",
    "### 1. `model=\"gemini-2.0-flash\"`\n",
    "**What it is:** The specific AI model we want to use  \n",
    "**Simple explanation:** Think of this like choosing which \"brain\" the AI should use. Different models have different capabilities and speeds.\n",
    "- `gemini-2.0-flash` is a fast and efficient model from Google\n",
    "- Other options: `gemini-2.5-pro`, `gemini-2.5-flash`, etc.\n",
    "\n",
    "**Example:** Just like you might choose between a calculator or a computer for math - both work, but have different speeds and capabilities!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `temperature=0`\n",
    "**What it is:** Controls how creative or random the AI's responses are  \n",
    "**Range:** 0.0 to 2.0  \n",
    "**Simple explanation:** \n",
    "- **`temperature=0`** ‚Üí Very predictable, same answer every time (like a calculator)\n",
    "- **`temperature=1`** ‚Üí Balanced, some creativity\n",
    "- **`temperature=2`** ‚Üí Very creative, different answers each time (like a creative writer)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# temperature=0: \"Hello! How can I help you?\"\n",
    "# temperature=1: \"Hello there! How may I assist you today?\"\n",
    "# temperature=2: \"Greetings! What exciting question can I help you explore?\"\n",
    "```\n",
    "\n",
    "**When to use what:**\n",
    "- Use **0** for factual answers (math, science, coding)\n",
    "- Use **0.7-1.0** for creative writing, brainstorming\n",
    "- Use **1.5-2.0** for maximum creativity (stories, poems)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `max_tokens=None`\n",
    "**What it is:** Maximum number of words/pieces the AI can generate  \n",
    "**Simple explanation:** This limits how long the AI's response can be.\n",
    "- `None` = No limit (AI decides based on the question)\n",
    "- `100` = Short response (~75 words)\n",
    "- `1000` = Long response (~750 words)\n",
    "\n",
    "**Why it matters:**\n",
    "- Prevents extremely long responses\n",
    "- Saves API costs (you pay per token)\n",
    "- Controls response length\n",
    "\n",
    "**Real-world analogy:** Like telling someone \"explain in 100 words or less\" vs \"explain in detail\"\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `timeout=None`\n",
    "**What it is:** How long to wait for the AI to respond (in seconds)  \n",
    "**Simple explanation:** If the AI takes too long, stop waiting and show an error.\n",
    "- `None` = Wait forever (not recommended for production)\n",
    "- `30` = Wait maximum 30 seconds\n",
    "- `60` = Wait maximum 1 minute\n",
    "\n",
    "**Example scenario:** If your internet is slow or the AI service is busy, this prevents your program from hanging forever.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `max_retries=2`\n",
    "**What it is:** How many times to try again if something fails  \n",
    "**Simple explanation:** If the request fails (network error, server busy), automatically try again.\n",
    "- `0` = Don't retry, fail immediately\n",
    "- `2` = Try 2 more times before giving up (total 3 attempts)\n",
    "- `5` = Try 5 more times (total 6 attempts)\n",
    "\n",
    "**Real-world example:**\n",
    "```\n",
    "1st attempt: ‚ùå Server busy\n",
    "2nd attempt: ‚ùå Network timeout\n",
    "3rd attempt: ‚úÖ Success!\n",
    "```\n",
    "\n",
    "**Why it's useful:** Makes your code more reliable by handling temporary network issues.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `api_key=google_api_key`\n",
    "**What it is:** Your personal password to access Google's AI service  \n",
    "**Simple explanation:** Like a key to unlock the AI service - proves you have permission to use it.\n",
    "\n",
    "**Important notes:**\n",
    "- üîí Keep this SECRET! Never share it publicly\n",
    "- üí∞ Tracks your usage for billing\n",
    "- üé´ Get it from: [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "**Security tip:** In real projects, store this in environment variables, not directly in code!\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Quick Summary Table\n",
    "\n",
    "| Parameter | Purpose | Common Values |\n",
    "|-----------|---------|---------------|\n",
    "| `model` | Which AI brain to use | `\"gemini-2.0-flash\"`, `\"gemini-pro\"` |\n",
    "| `temperature` | Creativity level | `0` (factual) to `2` (creative) |\n",
    "| `max_tokens` | Response length limit | `None`, `100`, `500`, `1000` |\n",
    "| `timeout` | Wait time limit | `None`, `30`, `60` (seconds) |\n",
    "| `max_retries` | Retry attempts | `0`, `2`, `5` |\n",
    "| `api_key` | Your access key | Your secret key string |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Best Practices for Students\n",
    "\n",
    "1. **Start simple:** Use default values (`temperature=0`, `max_retries=2`)\n",
    "2. **Experiment:** Try different temperatures to see how responses change\n",
    "3. **Set limits:** Use `max_tokens` to control costs when learning\n",
    "4. **Be secure:** Never commit API keys to GitHub or share them\n",
    "5. **Handle errors:** Use `max_retries` to make your code more robust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e8f01",
   "metadata": {},
   "source": [
    "# üß™ Hands-On Experiments\n",
    "\n",
    "Now let's see these parameters in action! Run each experiment below to see the differences.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8999c",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Temperature Experiments\n",
    "\n",
    "### Experiment 1: Temperature = 0 (Factual & Deterministic)\n",
    "**Use Case:** Math problems, factual questions, coding help\n",
    "\n",
    "This will give the same answer every time - perfect for when you need consistency!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d837633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 0 - Very Factual\n",
    "llm_temp_0 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "prompt = \"Write a short greeting message for a new user visiting our website.\"\n",
    "response_0 = llm_temp_0.invoke(prompt)\n",
    "print(\"üîµ Temperature = 0 (Factual):\")\n",
    "print(response_0.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5856f1",
   "metadata": {},
   "source": [
    "### Experiment 2: Temperature = 0.7 (Balanced)\n",
    "**Use Case:** General conversations, customer support, Q&A\n",
    "\n",
    "This provides a good balance between consistency and variety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02456713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 0.7 - Balanced\n",
    "llm_temp_07 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_07 = llm_temp_07.invoke(prompt)\n",
    "print(\"üü¢ Temperature = 0.7 (Balanced):\")\n",
    "print(response_07.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd46c0",
   "metadata": {},
   "source": [
    "### Experiment 3: Temperature = 1.5 (Very Creative)\n",
    "**Use Case:** Creative writing, brainstorming, story generation\n",
    "\n",
    "This will give more creative and varied responses!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011c8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 1.5 - Very Creative\n",
    "llm_temp_15 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=1.5,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_15 = llm_temp_15.invoke(prompt)\n",
    "print(\"üü° Temperature = 1.5 (Creative):\")\n",
    "print(response_15.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d92408",
   "metadata": {},
   "source": [
    "### Experiment 4: Temperature = 2.0 - Multiple Runs (Showing Randomness)\n",
    "**Use Case:** Maximum creativity for artistic content, diverse idea generation\n",
    "\n",
    "Run this cell multiple times to see how different the responses can be!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e1ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 2.0 - Maximum Creativity (try running this multiple times!)\n",
    "llm_temp_20 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=2.0,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "print(\"üî¥ Temperature = 2.0 (Maximum Creativity):\")\n",
    "print(\"Running 3 times to show variety:\\n\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    response_20 = llm_temp_20.invoke(prompt)\n",
    "    print(f\"Attempt {i}:\")\n",
    "    print(response_20.content)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe9eda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Max Tokens Experiments\n",
    "\n",
    "Now let's see how `max_tokens` controls the length of responses!\n",
    "\n",
    "**Note:** ~1 token ‚âà 0.75 words, so 100 tokens ‚âà 75 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9feb4d2",
   "metadata": {},
   "source": [
    "### Experiment 5: max_tokens = 50 (Very Short Response)\n",
    "**Use Case:** Quick answers, notifications, short summaries\n",
    "\n",
    "Perfect when you need brief, to-the-point responses!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716872f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens = 50 - Very Short\n",
    "llm_tokens_50 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=50,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "long_prompt = \"Explain what artificial intelligence is and how it's used in everyday life.\"\n",
    "response_50 = llm_tokens_50.invoke(long_prompt)\n",
    "\n",
    "print(\"üìù Max Tokens = 50 (Very Short):\")\n",
    "print(response_50.content)\n",
    "print(f\"\\nüìä Tokens used: {response_50.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da46240",
   "metadata": {},
   "source": [
    "### Experiment 6: max_tokens = 200 (Medium Response)\n",
    "**Use Case:** Explanations, descriptions, short articles\n",
    "\n",
    "Good balance between detail and brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens = 200 - Medium Length\n",
    "llm_tokens_200 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_200 = llm_tokens_200.invoke(long_prompt)\n",
    "\n",
    "print(\"üìÑ Max Tokens = 200 (Medium):\")\n",
    "print(response_200.content)\n",
    "print(f\"\\nüìä Tokens used: {response_200.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabeeafb",
   "metadata": {},
   "source": [
    "### Experiment 7: max_tokens = None (No Limit)\n",
    "**Use Case:** Detailed explanations, essays, comprehensive answers\n",
    "\n",
    "Let the AI decide how long the response should be based on the question!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f58f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_tokens = None - No Limit\n",
    "llm_tokens_none = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_none = llm_tokens_none.invoke(long_prompt)\n",
    "\n",
    "print(\"üìö Max Tokens = None (No Limit):\")\n",
    "print(response_none.content)\n",
    "print(f\"\\nüìä Tokens used: {response_none.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a245a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Compare All Max Token Settings Side-by-Side\n",
    "\n",
    "Run this cell to see all three responses together and compare their lengths!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35329dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Response Lengths with Different max_tokens\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìù 50 tokens  ‚Üí {response_50.usage_metadata['output_tokens']} tokens used\")\n",
    "print(f\"üìÑ 200 tokens ‚Üí {response_200.usage_metadata['output_tokens']} tokens used\")\n",
    "print(f\"üìö No limit   ‚Üí {response_none.usage_metadata['output_tokens']} tokens used\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"Lower max_tokens = Shorter, faster, cheaper responses\")\n",
    "print(\"Higher max_tokens = Longer, more detailed, costlier responses\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5eaae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì What You Learned from These Experiments\n",
    "\n",
    "### Temperature Effects:\n",
    "- **Temperature = 0** ‚Üí Same output every time (deterministic)\n",
    "- **Temperature = 0.7** ‚Üí Slight variations (balanced)\n",
    "- **Temperature = 1.5** ‚Üí More creative variations\n",
    "- **Temperature = 2.0** ‚Üí Maximum creativity, very different each time\n",
    "\n",
    "### Max Tokens Effects:\n",
    "- **50 tokens** ‚Üí Very brief, cuts off mid-response (~37 words)\n",
    "- **200 tokens** ‚Üí Good for short explanations (~150 words)\n",
    "- **None (no limit)** ‚Üí Full response based on what's needed\n",
    "\n",
    "### üí° Pro Tips for Your Projects:\n",
    "1. **For factual tasks** (homework, calculations): Use `temperature=0`\n",
    "2. **For creative tasks** (stories, brainstorming): Use `temperature=1.0-2.0`\n",
    "3. **To save costs**: Set appropriate `max_tokens` limits\n",
    "4. **For production apps**: Always set `timeout` and `max_retries` for reliability\n",
    "\n",
    "---\n",
    "\n",
    "**üé¨ Tutorial Tip:** Try modifying the prompts and running the experiments again to see how different inputs affect the outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f32084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.content)\n",
    "print(output.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7ddda",
   "metadata": {},
   "source": [
    "# Difference Between `output.content` vs `output.text`\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "### `output.content`\n",
    "- **Type**: Can be either `str` OR `list[str | dict]`\n",
    "- **Contains**: The raw, complete content of the message as returned by the LLM\n",
    "- **Use case**: When you need the full message structure, including multimodal content (text, images, tool calls, etc.)\n",
    "\n",
    "### `output.text`\n",
    "- **Type**: Always returns a `str` (specifically a `TextAccessor` that behaves like a string)\n",
    "- **Contains**: Extracts and concatenates **only the text portions** from the content\n",
    "- **Use case**: When you only need the text output, especially for multimodal responses\n",
    "\n",
    "---\n",
    "\n",
    "## When They're the Same\n",
    "In simple text-only responses (like our \"Hello, world!\" example), both return identical values:\n",
    "```python\n",
    "output.content == output.text  # True for simple text responses\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When They're Different\n",
    "\n",
    "### 1. **Multimodal Responses** (text + images)\n",
    "```python\n",
    "# content would be:\n",
    "[\n",
    "    {\"type\": \"text\", \"text\": \"Here's the image you requested:\"},\n",
    "    {\"type\": \"image_url\", \"url\": \"https://...\"}\n",
    "]\n",
    "\n",
    "# text extracts only:\n",
    "\"Here's the image you requested:\"\n",
    "```\n",
    "\n",
    "### 2. **Tool Calls / Function Calling**\n",
    "```python\n",
    "# content might be:\n",
    "[\n",
    "    {\"type\": \"text\", \"text\": \"Let me check that for you\"},\n",
    "    {\"type\": \"tool_use\", \"name\": \"search\", \"args\": {...}}\n",
    "]\n",
    "\n",
    "# text extracts only:\n",
    "\"Let me check that for you\"\n",
    "```\n",
    "\n",
    "### 3. **Multiple Text Blocks**\n",
    "```python\n",
    "# content could be:\n",
    "[\n",
    "    \"First paragraph\",\n",
    "    {\"type\": \"text\", \"text\": \"Second paragraph\"},\n",
    "    \"Third paragraph\"\n",
    "]\n",
    "\n",
    "# text joins them:\n",
    "\"First paragraphSecond paragraphThird paragraph\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendation\n",
    "- ‚úÖ Use **`.text`** for most cases - it's safe and always gives you a string\n",
    "- ‚úÖ Use **`.content`** when you need to access non-text elements (images, tool calls, structured data)\n",
    "- ‚ÑπÔ∏è In simple text-only responses, they're functionally identical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724ce0d",
   "metadata": {},
   "source": [
    "# üí∞ Understanding Usage Metadata - Tracking API Costs\n",
    "\n",
    "When you make a request to the AI, it returns `usage_metadata` that tells you exactly how many tokens were used. This is **super important** for understanding costs!\n",
    "\n",
    "Let's examine what each field means:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1452f3",
   "metadata": {},
   "source": [
    "## üìä Breaking Down the Usage Metadata\n",
    "\n",
    "Here's what you'll see when you print `output.usage_metadata`:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_tokens': 4,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 15,\n",
    "    'input_token_details': {'cache_read': 0}\n",
    "}\n",
    "```\n",
    "\n",
    "Let's understand each field:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `input_tokens` (Number of tokens in your question)\n",
    "\n",
    "**What it is:** The number of tokens in the message **you sent** to the AI\n",
    "\n",
    "**Example:** When you send \"Hello, world!\" ‚Üí This gets broken into 4 tokens\n",
    "- Token 1: \"Hello\"\n",
    "- Token 2: \",\"\n",
    "- Token 3: \" world\"\n",
    "- Token 4: \"!\"\n",
    "\n",
    "**Why it matters:**\n",
    "- Longer questions = More input tokens\n",
    "- You pay for input tokens (though usually cheaper than output tokens)\n",
    "- Some models have input token limits (e.g., max 8,000 or 32,000 tokens)\n",
    "\n",
    "**Think of it as:** The \"question length\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `output_tokens` (Number of tokens in the AI's response)\n",
    "\n",
    "**What it is:** The number of tokens in the response **the AI generated**\n",
    "\n",
    "**Example:** AI responds: \"Hello there! How can I help you today?\" ‚Üí 11 tokens\n",
    "\n",
    "**Why it matters:**\n",
    "- This is what you **pay the most** for (output tokens cost more than input)\n",
    "- Longer responses = Higher costs\n",
    "- This is what `max_tokens` parameter controls\n",
    "\n",
    "**üí° Cost Tip:** If you want to save money, use `max_tokens` to limit the response length!\n",
    "\n",
    "**Think of it as:** The \"answer length\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `total_tokens` (Total tokens used)\n",
    "\n",
    "**What it is:** Simply `input_tokens + output_tokens`\n",
    "\n",
    "**Formula:** `total_tokens = input_tokens + output_tokens`\n",
    "\n",
    "**Example:** 4 (input) + 11 (output) = 15 (total)\n",
    "\n",
    "**Why it matters:**\n",
    "- Quick way to see overall usage\n",
    "- Some API pricing is based on total tokens\n",
    "- Helps track your monthly usage limits\n",
    "\n",
    "**Think of it as:** The \"complete conversation size\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `input_token_details` (Advanced tracking)\n",
    "\n",
    "**What it is:** Extra information about the input tokens\n",
    "\n",
    "**Structure:**\n",
    "```python\n",
    "'input_token_details': {\n",
    "    'cache_read': 0  # Tokens read from cache\n",
    "}\n",
    "```\n",
    "\n",
    "#### `cache_read` (Cached tokens - Advanced Feature)\n",
    "\n",
    "**What it is:** Number of tokens that were retrieved from the cache instead of being processed again\n",
    "\n",
    "**How it works:**\n",
    "- If you send the **same prompt multiple times**, Google might cache it\n",
    "- Cached tokens are **cheaper** or sometimes **free**!\n",
    "- `cache_read: 0` means no caching happened (first time asking)\n",
    "- `cache_read: 50` would mean 50 tokens were reused from cache\n",
    "\n",
    "**Example scenario:**\n",
    "```python\n",
    "# First request - no cache\n",
    "output1 = llm.invoke(\"What is Python?\")\n",
    "# input_token_details: {'cache_read': 0}\n",
    "\n",
    "# Second request with same context - might use cache\n",
    "output2 = llm.invoke(\"What is Python?\")  \n",
    "# input_token_details: {'cache_read': 4}  # Reused from cache!\n",
    "```\n",
    "\n",
    "**Think of it as:** The \"money saved by recycling\" counter\n",
    "\n",
    "---\n",
    "\n",
    "## üíµ Real-World Pricing Example\n",
    "\n",
    "Let's say Google charges (hypothetical rates):\n",
    "- Input tokens: $0.01 per 1,000 tokens\n",
    "- Output tokens: $0.03 per 1,000 tokens\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "{\n",
    "    'input_tokens': 4,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 15\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost calculation:**\n",
    "- Input cost: (4 / 1,000) √ó $0.01 = $0.00004\n",
    "- Output cost: (11 / 1,000) √ó $0.03 = $0.00033\n",
    "- **Total cost: $0.00037** (less than a penny!)\n",
    "\n",
    "But if you make 1,000 requests:\n",
    "- Total cost: $0.37\n",
    "- Total cost for 10,000 requests: $3.70\n",
    "\n",
    "**This is why tracking tokens matters!** üí∞\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Practical Tips for Students\n",
    "\n",
    "### 1. **Monitor Your Usage**\n",
    "Always check `usage_metadata` to see how many tokens you're using:\n",
    "```python\n",
    "response = llm.invoke(\"Your question here\")\n",
    "print(f\"Cost estimate: {response.usage_metadata['total_tokens']} tokens\")\n",
    "```\n",
    "\n",
    "### 2. **Optimize for Cost**\n",
    "- Use shorter, clearer prompts (reduces input tokens)\n",
    "- Set `max_tokens` to limit output length\n",
    "- Use `temperature=0` for consistent, often shorter responses\n",
    "\n",
    "### 3. **Watch for Token Limits**\n",
    "- If your prompt + response > model limit, you'll get an error\n",
    "- Example: 8K token limit means `input_tokens + output_tokens ‚â§ 8,000`\n",
    "\n",
    "### 4. **Free Tier Management**\n",
    "Most AI services give free tokens per month:\n",
    "- Google Gemini: Often 50-100 requests/day free\n",
    "- Track your daily usage to stay within limits\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Quick Token Rules of Thumb\n",
    "\n",
    "| Text Length | Approximate Tokens |\n",
    "|-------------|-------------------|\n",
    "| 1 word | ~1-2 tokens |\n",
    "| 1 sentence (10 words) | ~13-15 tokens |\n",
    "| 1 paragraph (100 words) | ~130-150 tokens |\n",
    "| 1 page (500 words) | ~650-750 tokens |\n",
    "\n",
    "**Remember:** Tokens ‚â† Words! Punctuation, spaces, and special characters count too!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Try This Exercise!\n",
    "\n",
    "Run the next cell to see the usage metadata for our \"Hello, world!\" example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e7b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5da76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Let's Analyze This Output!\n",
    "\n",
    "From the output above, we can see:\n",
    "\n",
    "1. **Input Tokens = 4**\n",
    "   - Our prompt was: \"Hello, world!\"\n",
    "   - This simple phrase = 4 tokens\n",
    "\n",
    "2. **Output Tokens = 11**\n",
    "   - AI's response: \"Hello there! How can I help you today?\"\n",
    "   - This response = 11 tokens\n",
    "\n",
    "3. **Total Tokens = 15**\n",
    "   - Total usage: 4 + 11 = 15 tokens\n",
    "\n",
    "4. **Cache Read = 0**\n",
    "   - This was a fresh request (not cached)\n",
    "\n",
    "### üí° Key Insight:\n",
    "Notice that the AI's response (11 tokens) was **almost 3x longer** than our question (4 tokens)! This is why output tokens cost more - the AI does more work generating responses than processing your input.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experiment Idea:\n",
    "\n",
    "Try running this with different prompts and see how token counts change:\n",
    "\n",
    "```python\n",
    "# Short prompt\n",
    "short = llm.invoke(\"Hi\")\n",
    "\n",
    "# Medium prompt  \n",
    "medium = llm.invoke(\"Can you explain what machine learning is?\")\n",
    "\n",
    "# Long prompt\n",
    "long = llm.invoke(\"I need a detailed explanation of how neural networks work, including backpropagation, activation functions, and gradient descent.\")\n",
    "\n",
    "# Compare their usage\n",
    "print(f\"Short: {short.usage_metadata['total_tokens']} tokens\")\n",
    "print(f\"Medium: {medium.usage_metadata['total_tokens']} tokens\")\n",
    "print(f\"Long: {long.usage_metadata['total_tokens']} tokens\")\n",
    "```\n",
    "\n",
    "**What do you think will happen?** ü§î\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3c59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
