{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2978772f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "\n",
    "google_api_key = \"\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    api_key=google_api_key,\n",
    ")\n",
    "output = llm.invoke(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4418ecee",
   "metadata": {},
   "source": [
    "# Understanding ChatGoogleGenerativeAI Parameters\n",
    "\n",
    "When we create an AI chatbot using Google's Gemini model, we need to configure it with several parameters. Let's understand each one in simple terms:\n",
    "\n",
    "---\n",
    "\n",
    "## ü§ñ Core Parameters\n",
    "\n",
    "### 1. `model=\"gemini-2.0-flash\"`\n",
    "**What it is:** The specific AI model we want to use  \n",
    "**Simple explanation:** Think of this like choosing which \"brain\" the AI should use. Different models have different capabilities and speeds.\n",
    "- `gemini-2.0-flash` is a fast and efficient model from Google\n",
    "- Other options: `gemini-2.5-pro`, `gemini-2.5-flash`, etc.\n",
    "\n",
    "**Example:** Just like you might choose between a calculator or a computer for math - both work, but have different speeds and capabilities!\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `temperature=0`\n",
    "**What it is:** Controls how creative or random the AI's responses are  \n",
    "**Range:** 0.0 to 2.0  \n",
    "**Simple explanation:** \n",
    "- **`temperature=0`** ‚Üí Very predictable, same answer every time (like a calculator)\n",
    "- **`temperature=1`** ‚Üí Balanced, some creativity\n",
    "- **`temperature=2`** ‚Üí Very creative, different answers each time (like a creative writer)\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "# temperature=0: \"Hello! How can I help you?\"\n",
    "# temperature=1: \"Hello there! How may I assist you today?\"\n",
    "# temperature=2: \"Greetings! What exciting question can I help you explore?\"\n",
    "```\n",
    "\n",
    "**When to use what:**\n",
    "- Use **0** for factual answers (math, science, coding)\n",
    "- Use **0.7-1.0** for creative writing, brainstorming\n",
    "- Use **1.5-2.0** for maximum creativity (stories, poems)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `max_tokens=None`\n",
    "**What it is:** Maximum number of words/pieces the AI can generate  \n",
    "**Simple explanation:** This limits how long the AI's response can be.\n",
    "- `None` = No limit (AI decides based on the question)\n",
    "- `100` = Short response (~75 words)\n",
    "- `1000` = Long response (~750 words)\n",
    "\n",
    "**Why it matters:**\n",
    "- Prevents extremely long responses\n",
    "- Saves API costs (you pay per token)\n",
    "- Controls response length\n",
    "\n",
    "**Real-world analogy:** Like telling someone \"explain in 100 words or less\" vs \"explain in detail\"\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `timeout=None`\n",
    "**What it is:** How long to wait for the AI to respond (in seconds)  \n",
    "**Simple explanation:** If the AI takes too long, stop waiting and show an error.\n",
    "- `None` = Wait forever (not recommended for production)\n",
    "- `30` = Wait maximum 30 seconds\n",
    "- `60` = Wait maximum 1 minute\n",
    "\n",
    "**Example scenario:** If your internet is slow or the AI service is busy, this prevents your program from hanging forever.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. `max_retries=2`\n",
    "**What it is:** How many times to try again if something fails  \n",
    "**Simple explanation:** If the request fails (network error, server busy), automatically try again.\n",
    "- `0` = Don't retry, fail immediately\n",
    "- `2` = Try 2 more times before giving up (total 3 attempts)\n",
    "- `5` = Try 5 more times (total 6 attempts)\n",
    "\n",
    "**Real-world example:**\n",
    "```\n",
    "1st attempt: ‚ùå Server busy\n",
    "2nd attempt: ‚ùå Network timeout\n",
    "3rd attempt: ‚úÖ Success!\n",
    "```\n",
    "\n",
    "**Why it's useful:** Makes your code more reliable by handling temporary network issues.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. `api_key=google_api_key`\n",
    "**What it is:** Your personal password to access Google's AI service  \n",
    "**Simple explanation:** Like a key to unlock the AI service - proves you have permission to use it.\n",
    "\n",
    "**Important notes:**\n",
    "- üîí Keep this SECRET! Never share it publicly\n",
    "- üí∞ Tracks your usage for billing\n",
    "- üé´ Get it from: [Google AI Studio](https://aistudio.google.com/app/apikey)\n",
    "\n",
    "**Security tip:** In real projects, store this in environment variables, not directly in code!\n",
    "\n",
    "---\n",
    "\n",
    "## üìù Quick Summary Table\n",
    "\n",
    "| Parameter | Purpose | Common Values |\n",
    "|-----------|---------|---------------|\n",
    "| `model` | Which AI brain to use | `\"gemini-2.0-flash\"`, `\"gemini-pro\"` |\n",
    "| `temperature` | Creativity level | `0` (factual) to `2` (creative) |\n",
    "| `max_tokens` | Response length limit | `None`, `100`, `500`, `1000` |\n",
    "| `timeout` | Wait time limit | `None`, `30`, `60` (seconds) |\n",
    "| `max_retries` | Retry attempts | `0`, `2`, `5` |\n",
    "| `api_key` | Your access key | Your secret key string |\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Best Practices for Students\n",
    "\n",
    "1. **Start simple:** Use default values (`temperature=0`, `max_retries=2`)\n",
    "2. **Experiment:** Try different temperatures to see how responses change\n",
    "3. **Set limits:** Use `max_tokens` to control costs when learning\n",
    "4. **Be secure:** Never commit API keys to GitHub or share them\n",
    "5. **Handle errors:** Use `max_retries` to make your code more robust\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13e8f01",
   "metadata": {},
   "source": [
    "# üß™ Hands-On Experiments\n",
    "\n",
    "Now let's see these parameters in action! Run each experiment below to see the differences.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d8999c",
   "metadata": {},
   "source": [
    "## üå°Ô∏è Temperature Experiments\n",
    "\n",
    "### Experiment 1: Temperature = 0 (Factual & Deterministic)\n",
    "**Use Case:** Math problems, factual questions, coding help\n",
    "\n",
    "This will give the same answer every time - perfect for when you need consistency!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d837633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Temperature = 0 (Factual):\n",
      "Welcome to our website! We're so glad you're here. Feel free to explore and discover what we have to offer. Happy browsing!\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature = 0 - Very Factual\n",
    "llm_temp_0 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "prompt = \"Write a short greeting message for a new user visiting our website.\"\n",
    "response_0 = llm_temp_0.invoke(prompt)\n",
    "print(\"üîµ Temperature = 0 (Factual):\")\n",
    "print(response_0.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5856f1",
   "metadata": {},
   "source": [
    "### Experiment 2: Temperature = 0.7 (Balanced)\n",
    "**Use Case:** General conversations, customer support, Q&A\n",
    "\n",
    "This provides a good balance between consistency and variety.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02456713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü¢ Temperature = 0.7 (Balanced):\n",
      "Hi there! Welcome to our website! We're so glad you're here. We hope you find everything you're looking for. Feel free to explore!\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature = 0.7 - Balanced\n",
    "llm_temp_07 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_07 = llm_temp_07.invoke(prompt)\n",
    "print(\"üü¢ Temperature = 0.7 (Balanced):\")\n",
    "print(response_07.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bd46c0",
   "metadata": {},
   "source": [
    "### Experiment 3: Temperature = 1.5 (Very Creative)\n",
    "**Use Case:** Creative writing, brainstorming, story generation\n",
    "\n",
    "This will give more creative and varied responses!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "011c8582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üü° Temperature = 1.5 (Creative):\n",
      "Welcome to our site! We're so glad you're here. Feel free to explore and let us know if you have any questions. Enjoy!\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature = 1.5 - Very Creative\n",
    "llm_temp_15 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=1.5,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_15 = llm_temp_15.invoke(prompt)\n",
    "print(\"üü° Temperature = 1.5 (Creative):\")\n",
    "print(response_15.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d92408",
   "metadata": {},
   "source": [
    "### Experiment 4: Temperature = 2.0 - Multiple Runs (Showing Randomness)\n",
    "**Use Case:** Maximum creativity for artistic content, diverse idea generation\n",
    "\n",
    "Run this cell multiple times to see how different the responses can be!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb8e1ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¥ Temperature = 2.0 (Maximum Creativity):\n",
      "Running 3 times to show variety:\n",
      "\n",
      "Attempt 1:\n",
      "Welcome to our website! We're excited to have you join us. Take a look around and discover what we have to offer. We hope you enjoy your visit!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Attempt 2:\n",
      "**Option 1 (Simple & Friendly):**\n",
      "\n",
      "> Welcome to our website! We're so glad you're here. Take a look around and explore!\n",
      "\n",
      "**Option 2 (Slightly More Informative):**\n",
      "\n",
      "> Hello and welcome! We're happy to have you visiting. Feel free to browse our [mention key areas, e.g., products, blog, services] to learn more.\n",
      "\n",
      "**Option 3 (Focusing on User Benefit):**\n",
      "\n",
      "> Welcome! We're thrilled to have you join us. Get ready to discover [mention the value they get from the site, e.g., amazing deals, insightful information, a helpful community].\n",
      "\n",
      "**Option 4 (More Personalized, if possible):**\n",
      "\n",
      "> Welcome [User's Name or Visitor]! We're excited you're checking us out. Let us know if you have any questions.\n",
      "\n",
      "**Choose the option that best fits your website's tone and purpose!**\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Attempt 3:\n",
      "**Option 1 (Simple & Welcoming):**\n",
      "\n",
      "> Welcome to our website! We're happy to have you.\n",
      "\n",
      "**Option 2 (Slightly more engaging):**\n",
      "\n",
      "> Hey there! Welcome! We're so glad you're visiting. Take a look around and let us know if you have any questions.\n",
      "\n",
      "**Option 3 (If you know what they're looking for):**\n",
      "\n",
      "> Welcome! We hope you find exactly what you're looking for here.  Happy exploring!\n",
      "\n",
      "**Option 4 (Short and personalized):**\n",
      "\n",
      "> Welcome aboard! Get ready to [mention website's benefit, e.g., learn new things, find great deals, etc.].\n",
      "\n",
      "**Which option is best depends on your brand voice and what your website offers.**\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Temperature = 2.0 - Maximum Creativity (try running this multiple times!)\n",
    "llm_temp_20 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=2.0,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "print(\"üî¥ Temperature = 2.0 (Maximum Creativity):\")\n",
    "print(\"Running 3 times to show variety:\\n\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    response_20 = llm_temp_20.invoke(prompt)\n",
    "    print(f\"Attempt {i}:\")\n",
    "    print(response_20.content)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe9eda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìè Max Tokens Experiments\n",
    "\n",
    "Now let's see how `max_tokens` controls the length of responses!\n",
    "\n",
    "**Note:** ~1 token ‚âà 0.75 words, so 100 tokens ‚âà 75 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9feb4d2",
   "metadata": {},
   "source": [
    "### Experiment 5: max_tokens = 50 (Very Short Response)\n",
    "**Use Case:** Quick answers, notifications, short summaries\n",
    "\n",
    "Perfect when you need brief, to-the-point responses!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "716872f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Max Tokens = 50 (Very Short):\n",
      "## What is Artificial Intelligence (AI)?\n",
      "\n",
      "Artificial Intelligence (AI) is essentially the ability of a computer or machine to mimic intelligent human behavior. It's about creating systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "*\n",
      "\n",
      "üìä Tokens used: 50\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_tokens = 50 - Very Short\n",
    "llm_tokens_50 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=50,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "long_prompt = \"Explain what artificial intelligence is and how it's used in everyday life.\"\n",
    "response_50 = llm_tokens_50.invoke(long_prompt)\n",
    "\n",
    "print(\"üìù Max Tokens = 50 (Very Short):\")\n",
    "print(response_50.content)\n",
    "print(f\"\\nüìä Tokens used: {response_50.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da46240",
   "metadata": {},
   "source": [
    "### Experiment 6: max_tokens = 200 (Medium Response)\n",
    "**Use Case:** Explanations, descriptions, short articles\n",
    "\n",
    "Good balance between detail and brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8eca66fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Max Tokens = 200 (Medium):\n",
      "## What is Artificial Intelligence (AI)?\n",
      "\n",
      "Artificial Intelligence (AI) is essentially the ability of a computer or machine to mimic human intelligence.  It involves creating systems that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "*   **Learning:** Acquiring information and rules for using that information.\n",
      "*   **Reasoning:** Drawing conclusions and making decisions based on available data.\n",
      "*   **Problem-solving:**  Identifying and finding solutions to challenges.\n",
      "*   **Perception:**  Understanding and interpreting sensory input (like images, sound, or text).\n",
      "*   **Natural Language Processing (NLP):**  Understanding and generating human language.\n",
      "\n",
      "AI is not a single technology, but rather a broad field encompassing various techniques and approaches, including:\n",
      "\n",
      "*   **Machine Learning (ML):**  A subset of AI where systems learn from data without being explicitly programmed.  Algorithms are trained on large datasets to identify patterns and make predictions.\n",
      "*   **Deep\n",
      "\n",
      "üìä Tokens used: 199\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_tokens = 200 - Medium Length\n",
    "llm_tokens_200 = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=200,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_200 = llm_tokens_200.invoke(long_prompt)\n",
    "\n",
    "print(\"üìÑ Max Tokens = 200 (Medium):\")\n",
    "print(response_200.content)\n",
    "print(f\"\\nüìä Tokens used: {response_200.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabeeafb",
   "metadata": {},
   "source": [
    "### Experiment 7: max_tokens = None (No Limit)\n",
    "**Use Case:** Detailed explanations, essays, comprehensive answers\n",
    "\n",
    "Let the AI decide how long the response should be based on the question!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10f58f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Max Tokens = None (No Limit):\n",
      "## What is Artificial Intelligence (AI)?\n",
      "\n",
      "Artificial Intelligence (AI) is essentially the **simulation of human intelligence processes by machines, especially computer systems.** It involves creating machines that can perform tasks that typically require human intelligence, such as:\n",
      "\n",
      "*   **Learning:** Acquiring information and rules for using the information.\n",
      "*   **Reasoning:** Using rules to reach conclusions, solve problems, and make decisions.\n",
      "*   **Problem-solving:** Identifying and finding solutions to complex issues.\n",
      "*   **Perception:** Understanding sensory input (like images, sound, and text).\n",
      "*   **Natural Language Processing (NLP):** Understanding and generating human language.\n",
      "\n",
      "AI isn't a single thing; it's a broad field encompassing various techniques and approaches. Some key types of AI include:\n",
      "\n",
      "*   **Machine Learning (ML):**  This is a subset of AI that focuses on enabling systems to learn from data without being explicitly programmed. Algorithms are trained on large datasets to identify patterns and make predictions.\n",
      "*   **Deep Learning (DL):** A more advanced form of ML that uses artificial neural networks with multiple layers (hence \"deep\") to analyze data and extract complex features.\n",
      "*   **Natural Language Processing (NLP):** Focuses on enabling computers to understand, interpret, and generate human language.\n",
      "*   **Computer Vision:**  Enables computers to \"see\" and interpret images and videos.\n",
      "*   **Robotics:**  Designing, constructing, operating, and applying robots, often incorporating AI to enable autonomous behavior.\n",
      "\n",
      "## AI in Everyday Life:\n",
      "\n",
      "AI is already deeply integrated into our daily routines, often without us even realizing it. Here are some examples:\n",
      "\n",
      "**1.  Smart Assistants (Siri, Alexa, Google Assistant):**\n",
      "\n",
      "*   **How it's used:** These virtual assistants use NLP to understand your voice commands and perform tasks like setting alarms, playing music, answering questions, making calls, and controlling smart home devices.\n",
      "*   **AI at play:** NLP (speech recognition, natural language understanding), machine learning (learning your preferences over time).\n",
      "\n",
      "**2.  Search Engines (Google, Bing):**\n",
      "\n",
      "*   **How it's used:**  AI powers the ranking of search results, auto-complete suggestions, and understanding the context of your search queries.\n",
      "*   **AI at play:** Machine learning (ranking algorithms), NLP (understanding search intent).\n",
      "\n",
      "**3.  Social Media (Facebook, Instagram, Twitter):**\n",
      "\n",
      "*   **How it's used:** AI is used for personalized content recommendations (what posts you see), targeted advertising, facial recognition, and content moderation (detecting hate speech or inappropriate content).\n",
      "*   **AI at play:** Machine learning (recommendation systems, ad targeting), computer vision (facial recognition), NLP (content moderation).\n",
      "\n",
      "**4.  Online Shopping (Amazon, eBay):**\n",
      "\n",
      "*   **How it's used:**  AI powers product recommendations, personalized shopping experiences, fraud detection, and supply chain optimization.\n",
      "*   **AI at play:** Machine learning (recommendation engines, fraud detection), NLP (product reviews analysis).\n",
      "\n",
      "**5.  Navigation Apps (Google Maps, Waze):**\n",
      "\n",
      "*   **How it's used:** AI is used to predict traffic patterns, suggest optimal routes, and estimate arrival times.\n",
      "*   **AI at play:** Machine learning (traffic prediction), data analysis (route optimization).\n",
      "\n",
      "**6.  Streaming Services (Netflix, Spotify):**\n",
      "\n",
      "*   **How it's used:**  AI powers personalized recommendations for movies, TV shows, and music based on your viewing/listening history.\n",
      "*   **AI at play:** Machine learning (recommendation systems).\n",
      "\n",
      "**7.  Banking and Finance:**\n",
      "\n",
      "*   **How it's used:**  AI is used for fraud detection, credit scoring, algorithmic trading, and customer service chatbots.\n",
      "*   **AI at play:** Machine learning (fraud detection, risk assessment), NLP (chatbots).\n",
      "\n",
      "**8.  Healthcare:**\n",
      "\n",
      "*   **How it's used:**  AI is used for disease diagnosis, drug discovery, personalized medicine, and robotic surgery.\n",
      "*   **AI at play:** Machine learning (image analysis for diagnosis), NLP (analyzing medical records), robotics.\n",
      "\n",
      "**9.  Spam Filters:**\n",
      "\n",
      "*   **How it's used:** AI helps identify and filter out spam emails based on patterns and content.\n",
      "*   **AI at play:** Machine learning (spam detection).\n",
      "\n",
      "**10. Autocorrect and Predictive Text:**\n",
      "\n",
      "*   **How it's used:** AI analyzes your typing patterns and predicts the next word you're likely to type, improving typing speed and accuracy.\n",
      "*   **AI at play:** NLP (language modeling).\n",
      "\n",
      "**In conclusion,** AI is rapidly evolving and becoming increasingly pervasive in our lives. While it's not always visible, it's quietly working behind the scenes to make our lives easier, more efficient, and more personalized.  As AI technology continues to advance, we can expect to see even more innovative applications in the future.\n",
      "\n",
      "üìä Tokens used: 1055\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max_tokens = None - No Limit\n",
    "llm_tokens_none = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    api_key=google_api_key\n",
    ")\n",
    "\n",
    "response_none = llm_tokens_none.invoke(long_prompt)\n",
    "\n",
    "print(\"üìö Max Tokens = None (No Limit):\")\n",
    "print(response_none.content)\n",
    "print(f\"\\nüìä Tokens used: {response_none.usage_metadata['output_tokens']}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a245a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Compare All Max Token Settings Side-by-Side\n",
    "\n",
    "Run this cell to see all three responses together and compare their lengths!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35329dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPARISON: Response Lengths with Different max_tokens\n",
      "============================================================\n",
      "\n",
      "üìù 50 tokens  ‚Üí 50 tokens used\n",
      "üìÑ 200 tokens ‚Üí 199 tokens used\n",
      "üìö No limit   ‚Üí 1055 tokens used\n",
      "\n",
      "============================================================\n",
      "\n",
      "üí° Key Takeaway:\n",
      "Lower max_tokens = Shorter, faster, cheaper responses\n",
      "Higher max_tokens = Longer, more detailed, costlier responses\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Side-by-side comparison\n",
    "print(\"=\"*60)\n",
    "print(\"COMPARISON: Response Lengths with Different max_tokens\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìù 50 tokens  ‚Üí {response_50.usage_metadata['output_tokens']} tokens used\")\n",
    "print(f\"üìÑ 200 tokens ‚Üí {response_200.usage_metadata['output_tokens']} tokens used\")\n",
    "print(f\"üìö No limit   ‚Üí {response_none.usage_metadata['output_tokens']} tokens used\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"Lower max_tokens = Shorter, faster, cheaper responses\")\n",
    "print(\"Higher max_tokens = Longer, more detailed, costlier responses\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e5eaae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì What You Learned from These Experiments\n",
    "\n",
    "### Temperature Effects:\n",
    "- **Temperature = 0** ‚Üí Same output every time (deterministic)\n",
    "- **Temperature = 0.7** ‚Üí Slight variations (balanced)\n",
    "- **Temperature = 1.5** ‚Üí More creative variations\n",
    "- **Temperature = 2.0** ‚Üí Maximum creativity, very different each time\n",
    "\n",
    "### Max Tokens Effects:\n",
    "- **50 tokens** ‚Üí Very brief, cuts off mid-response (~37 words)\n",
    "- **200 tokens** ‚Üí Good for short explanations (~150 words)\n",
    "- **None (no limit)** ‚Üí Full response based on what's needed\n",
    "\n",
    "### üí° Pro Tips for Your Projects:\n",
    "1. **For factual tasks** (homework, calculations): Use `temperature=0`\n",
    "2. **For creative tasks** (stories, brainstorming): Use `temperature=1.0-2.0`\n",
    "3. **To save costs**: Set appropriate `max_tokens` limits\n",
    "4. **For production apps**: Always set `timeout` and `max_retries` for reliability\n",
    "\n",
    "---\n",
    "\n",
    "**üé¨ Tutorial Tip:** Try modifying the prompts and running the experiments again to see how different inputs affect the outputs!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42f32084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n",
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "print(output.content)\n",
    "print(output.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7ddda",
   "metadata": {},
   "source": [
    "# Difference Between `output.content` vs `output.text`\n",
    "\n",
    "## Key Differences\n",
    "\n",
    "### `output.content`\n",
    "- **Type**: Can be either `str` OR `list[str | dict]`\n",
    "- **Contains**: The raw, complete content of the message as returned by the LLM\n",
    "- **Use case**: When you need the full message structure, including multimodal content (text, images, tool calls, etc.)\n",
    "\n",
    "### `output.text`\n",
    "- **Type**: Always returns a `str` (specifically a `TextAccessor` that behaves like a string)\n",
    "- **Contains**: Extracts and concatenates **only the text portions** from the content\n",
    "- **Use case**: When you only need the text output, especially for multimodal responses\n",
    "\n",
    "---\n",
    "\n",
    "## When They're the Same\n",
    "In simple text-only responses (like our \"Hello, world!\" example), both return identical values:\n",
    "```python\n",
    "output.content == output.text  # True for simple text responses\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## When They're Different\n",
    "\n",
    "### 1. **Multimodal Responses** (text + images)\n",
    "```python\n",
    "# content would be:\n",
    "[\n",
    "    {\"type\": \"text\", \"text\": \"Here's the image you requested:\"},\n",
    "    {\"type\": \"image_url\", \"url\": \"https://...\"}\n",
    "]\n",
    "\n",
    "# text extracts only:\n",
    "\"Here's the image you requested:\"\n",
    "```\n",
    "\n",
    "### 2. **Tool Calls / Function Calling**\n",
    "```python\n",
    "# content might be:\n",
    "[\n",
    "    {\"type\": \"text\", \"text\": \"Let me check that for you\"},\n",
    "    {\"type\": \"tool_use\", \"name\": \"search\", \"args\": {...}}\n",
    "]\n",
    "\n",
    "# text extracts only:\n",
    "\"Let me check that for you\"\n",
    "```\n",
    "\n",
    "### 3. **Multiple Text Blocks**\n",
    "```python\n",
    "# content could be:\n",
    "[\n",
    "    \"First paragraph\",\n",
    "    {\"type\": \"text\", \"text\": \"Second paragraph\"},\n",
    "    \"Third paragraph\"\n",
    "]\n",
    "\n",
    "# text joins them:\n",
    "\"First paragraphSecond paragraphThird paragraph\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Recommendation\n",
    "- ‚úÖ Use **`.text`** for most cases - it's safe and always gives you a string\n",
    "- ‚úÖ Use **`.content`** when you need to access non-text elements (images, tool calls, structured data)\n",
    "- ‚ÑπÔ∏è In simple text-only responses, they're functionally identical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5724ce0d",
   "metadata": {},
   "source": [
    "# üí∞ Understanding Usage Metadata - Tracking API Costs\n",
    "\n",
    "When you make a request to the AI, it returns `usage_metadata` that tells you exactly how many tokens were used. This is **super important** for understanding costs!\n",
    "\n",
    "Let's examine what each field means:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1452f3",
   "metadata": {},
   "source": [
    "## üìä Breaking Down the Usage Metadata\n",
    "\n",
    "Here's what you'll see when you print `output.usage_metadata`:\n",
    "\n",
    "```python\n",
    "{\n",
    "    'input_tokens': 4,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 15,\n",
    "    'input_token_details': {'cache_read': 0}\n",
    "}\n",
    "```\n",
    "\n",
    "Let's understand each field:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. `input_tokens` (Number of tokens in your question)\n",
    "\n",
    "**What it is:** The number of tokens in the message **you sent** to the AI\n",
    "\n",
    "**Example:** When you send \"Hello, world!\" ‚Üí This gets broken into 4 tokens\n",
    "- Token 1: \"Hello\"\n",
    "- Token 2: \",\"\n",
    "- Token 3: \" world\"\n",
    "- Token 4: \"!\"\n",
    "\n",
    "**Why it matters:**\n",
    "- Longer questions = More input tokens\n",
    "- You pay for input tokens (though usually cheaper than output tokens)\n",
    "- Some models have input token limits (e.g., max 8,000 or 32,000 tokens)\n",
    "\n",
    "**Think of it as:** The \"question length\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `output_tokens` (Number of tokens in the AI's response)\n",
    "\n",
    "**What it is:** The number of tokens in the response **the AI generated**\n",
    "\n",
    "**Example:** AI responds: \"Hello there! How can I help you today?\" ‚Üí 11 tokens\n",
    "\n",
    "**Why it matters:**\n",
    "- This is what you **pay the most** for (output tokens cost more than input)\n",
    "- Longer responses = Higher costs\n",
    "- This is what `max_tokens` parameter controls\n",
    "\n",
    "**üí° Cost Tip:** If you want to save money, use `max_tokens` to limit the response length!\n",
    "\n",
    "**Think of it as:** The \"answer length\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 3. `total_tokens` (Total tokens used)\n",
    "\n",
    "**What it is:** Simply `input_tokens + output_tokens`\n",
    "\n",
    "**Formula:** `total_tokens = input_tokens + output_tokens`\n",
    "\n",
    "**Example:** 4 (input) + 11 (output) = 15 (total)\n",
    "\n",
    "**Why it matters:**\n",
    "- Quick way to see overall usage\n",
    "- Some API pricing is based on total tokens\n",
    "- Helps track your monthly usage limits\n",
    "\n",
    "**Think of it as:** The \"complete conversation size\" counter\n",
    "\n",
    "---\n",
    "\n",
    "### 4. `input_token_details` (Advanced tracking)\n",
    "\n",
    "**What it is:** Extra information about the input tokens\n",
    "\n",
    "**Structure:**\n",
    "```python\n",
    "'input_token_details': {\n",
    "    'cache_read': 0  # Tokens read from cache\n",
    "}\n",
    "```\n",
    "\n",
    "#### `cache_read` (Cached tokens - Advanced Feature)\n",
    "\n",
    "**What it is:** Number of tokens that were retrieved from the cache instead of being processed again\n",
    "\n",
    "**How it works:**\n",
    "- If you send the **same prompt multiple times**, Google might cache it\n",
    "- Cached tokens are **cheaper** or sometimes **free**!\n",
    "- `cache_read: 0` means no caching happened (first time asking)\n",
    "- `cache_read: 50` would mean 50 tokens were reused from cache\n",
    "\n",
    "**Example scenario:**\n",
    "```python\n",
    "# First request - no cache\n",
    "output1 = llm.invoke(\"What is Python?\")\n",
    "# input_token_details: {'cache_read': 0}\n",
    "\n",
    "# Second request with same context - might use cache\n",
    "output2 = llm.invoke(\"What is Python?\")  \n",
    "# input_token_details: {'cache_read': 4}  # Reused from cache!\n",
    "```\n",
    "\n",
    "**Think of it as:** The \"money saved by recycling\" counter\n",
    "\n",
    "---\n",
    "\n",
    "## üíµ Real-World Pricing Example\n",
    "\n",
    "Let's say Google charges (hypothetical rates):\n",
    "- Input tokens: $0.01 per 1,000 tokens\n",
    "- Output tokens: $0.03 per 1,000 tokens\n",
    "\n",
    "For our example:\n",
    "```python\n",
    "{\n",
    "    'input_tokens': 4,\n",
    "    'output_tokens': 11,\n",
    "    'total_tokens': 15\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost calculation:**\n",
    "- Input cost: (4 / 1,000) √ó $0.01 = $0.00004\n",
    "- Output cost: (11 / 1,000) √ó $0.03 = $0.00033\n",
    "- **Total cost: $0.00037** (less than a penny!)\n",
    "\n",
    "But if you make 1,000 requests:\n",
    "- Total cost: $0.37\n",
    "- Total cost for 10,000 requests: $3.70\n",
    "\n",
    "**This is why tracking tokens matters!** üí∞\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Practical Tips for Students\n",
    "\n",
    "### 1. **Monitor Your Usage**\n",
    "Always check `usage_metadata` to see how many tokens you're using:\n",
    "```python\n",
    "response = llm.invoke(\"Your question here\")\n",
    "print(f\"Cost estimate: {response.usage_metadata['total_tokens']} tokens\")\n",
    "```\n",
    "\n",
    "### 2. **Optimize for Cost**\n",
    "- Use shorter, clearer prompts (reduces input tokens)\n",
    "- Set `max_tokens` to limit output length\n",
    "- Use `temperature=0` for consistent, often shorter responses\n",
    "\n",
    "### 3. **Watch for Token Limits**\n",
    "- If your prompt + response > model limit, you'll get an error\n",
    "- Example: 8K token limit means `input_tokens + output_tokens ‚â§ 8,000`\n",
    "\n",
    "### 4. **Free Tier Management**\n",
    "Most AI services give free tokens per month:\n",
    "- Google Gemini: Often 50-100 requests/day free\n",
    "- Track your daily usage to stay within limits\n",
    "\n",
    "---\n",
    "\n",
    "## üßÆ Quick Token Rules of Thumb\n",
    "\n",
    "| Text Length | Approximate Tokens |\n",
    "|-------------|-------------------|\n",
    "| 1 word | ~1-2 tokens |\n",
    "| 1 sentence (10 words) | ~13-15 tokens |\n",
    "| 1 paragraph (100 words) | ~130-150 tokens |\n",
    "| 1 page (500 words) | ~650-750 tokens |\n",
    "\n",
    "**Remember:** Tokens ‚â† Words! Punctuation, spaces, and special characters count too!\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Try This Exercise!\n",
    "\n",
    "Run the next cell to see the usage metadata for our \"Hello, world!\" example:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1089402",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "29e7b83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 4, 'output_tokens': 11, 'total_tokens': 15, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(output.usage_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc5da76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîç Let's Analyze This Output!\n",
    "\n",
    "From the output above, we can see:\n",
    "\n",
    "1. **Input Tokens = 4**\n",
    "   - Our prompt was: \"Hello, world!\"\n",
    "   - This simple phrase = 4 tokens\n",
    "\n",
    "2. **Output Tokens = 11**\n",
    "   - AI's response: \"Hello there! How can I help you today?\"\n",
    "   - This response = 11 tokens\n",
    "\n",
    "3. **Total Tokens = 15**\n",
    "   - Total usage: 4 + 11 = 15 tokens\n",
    "\n",
    "4. **Cache Read = 0**\n",
    "   - This was a fresh request (not cached)\n",
    "\n",
    "### üí° Key Insight:\n",
    "Notice that the AI's response (11 tokens) was **almost 3x longer** than our question (4 tokens)! This is why output tokens cost more - the AI does more work generating responses than processing your input.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Experiment Idea:\n",
    "\n",
    "Try running this with different prompts and see how token counts change:\n",
    "\n",
    "```python\n",
    "# Short prompt\n",
    "short = llm.invoke(\"Hi\")\n",
    "\n",
    "# Medium prompt  \n",
    "medium = llm.invoke(\"Can you explain what machine learning is?\")\n",
    "\n",
    "# Long prompt\n",
    "long = llm.invoke(\"I need a detailed explanation of how neural networks work, including backpropagation, activation functions, and gradient descent.\")\n",
    "\n",
    "# Compare their usage\n",
    "print(f\"Short: {short.usage_metadata['total_tokens']} tokens\")\n",
    "print(f\"Medium: {medium.usage_metadata['total_tokens']} tokens\")\n",
    "print(f\"Long: {long.usage_metadata['total_tokens']} tokens\")\n",
    "```\n",
    "\n",
    "**What do you think will happen?** ü§î\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 4, 'output_tokens': 597, 'total_tokens': 601, 'input_token_details': {'cache_read': 0}}\n",
      "{'input_tokens': 4, 'output_tokens': 597, 'total_tokens': 601, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
